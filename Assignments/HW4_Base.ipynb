{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Ensemble Methods and Decision Trees \n",
    "## CSCI 5622 - Spring 2019\n",
    "***\n",
    "**Name**: $<$Akash Iyengar\n",
    "$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11.59 PM on Wednesday, March 20**. Submit only this Jupyter notebook to Canvas.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your classmates and instructors, but **you must write all code and solutions on your own**, and list any people or sources consulted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "***\n",
    "Please do not change this class. We will use the MNIST dataset for this assignment. You have previously trained KNN, Logistic Regression on this dataset. You will now be using Ensemble methods and Decision Trees. This is a good opportunity to compare the results of different Machine Learning Algorithms on the dataset.\n",
    "\n",
    "This is a binary classification task. We have only included 3's and 8's for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreesAndEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    \n",
    "        X_train, y_train = train_set\n",
    "        X_valid, y_valid = valid_set\n",
    "\n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.X_train = X_train[np.logical_or( y_train==3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or( y_train==3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.X_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.X_train = self.X_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.X_valid = X_valid[np.logical_or( y_valid==3, y_valid == 8), :]\n",
    "        self.y_valid = y_valid[np.logical_or( y_valid==3, y_valid == 8)]\n",
    "        self.y_valid = np.array([1 if y == 8 else -1 for y in self.y_valid])\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"data/mnist.pklz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore this data and get comfortable with it before proceeding further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "Given a standard training set $D$ of size n, bagging generates $N$ new training sets $D_i$, roughly each of size n * ratio, by sampling from $D$ uniformly and with replacement. By sampling with replacement, some observations may be repeated in each $D_i$ The $N$ models are fitted using the above $N$ bootstraped samples and combined by averaging the output (for regression) or voting (for classification). \n",
    "\n",
    "-Source [Wiki](https://en.wikipedia.org/wiki/Bootstrap_aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bagging [25 points]\n",
    "***\n",
    "\n",
    "We've given you a skeleton of the class `BaggingClassifier` below which will train a classifier based on the decision trees as implemented by sklearn. Your tasks are as follows, please approach step by step to understand the code flow:\n",
    "* Implement `bootstrap` method which takes in two parameters (`X_train, y_train`) and returns a bootstrapped training set ($D_i$)\n",
    "* Implement `fit` method which takes in two parameters (`X_train, y_train`) and trains `N` number of base models on different bootstrap samples. You should call `bootstrap` method to get bootstrapped training data for each of your base model\n",
    "* Implement `voting` method which takes the predictions from learner trained on bootstrapped data points `y_hats` and returns final prediction as per majority rule. In case of ties, return either of the class randomly.\n",
    "* Implement `predict` method which takes in multiple data points and returns final prediction for each one of those. Please use the `voting` method to reach consensus on final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, ratio = 0.20, N = 20, base=DecisionTreeClassifier(max_depth=4)):\n",
    "        \"\"\"\n",
    "        Create a new BaggingClassifier\n",
    "        \n",
    "        Args:\n",
    "            base (BaseEstimator, optional): Sklearn implementation of decision tree\n",
    "            ratio: ratio of number of data points in subsampled data to the actual training data\n",
    "            N: number of base estimator in the ensemble\n",
    "        \n",
    "        Attributes:\n",
    "            base (estimator): Sklearn implementation of decision tree\n",
    "            N: Number of decision trees\n",
    "            learners: List of models trained on bootstrapped data sample\n",
    "        \"\"\"\n",
    "        \n",
    "        assert ratio <= 1.0, \"Cannot have ratio greater than one\"\n",
    "        self.base = base\n",
    "        self.ratio = ratio\n",
    "        self.N = N\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train Bagging Ensemble Classifier on data\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        #TODO: Implement functionality to fit models on the bootstrapped samples\n",
    "        # cloning sklearn models:\n",
    "        # from sklearn.base import clone\n",
    "        # h = clone(self.base)\n",
    "        from sklearn.base import clone\n",
    "        for i in range (0,self.N):\n",
    "            h=clone(self.base)\n",
    "            bt_X,bt_y= self.boostrap(X_train,y_train)\n",
    "            self.learners.append(h.fit(bt_X, bt_y))\n",
    "  \n",
    "        \n",
    "        \n",
    "    def boostrap(self, X_train, y_train):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): total size of the training data\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        n=len(X_train)\n",
    "        r_indi = np.random.choice(np.arange(n),size = int(n*self.ratio), replace = True)\n",
    "        bstrapped_X = X_train[r_indi]\n",
    "        bstrapped_y = y_train[r_indi]\n",
    "       \n",
    "        return bstrapped_X, bstrapped_y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        yhat = []\n",
    "        for di in X:\n",
    "            y_pdict = []\n",
    "            for learn in self.learners:\n",
    "                y_pdict.append(learn.predict([di]))\n",
    "            yhat.append(self.voting(y_pdict))\n",
    "            \n",
    "        return yhat\n",
    "        \"\"\"\n",
    "        BaggingClassifier prediction for data points in X\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns:\n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO: Using the individual classifiers trained predict the final prediction using voting mechanism\n",
    "    \n",
    "    \n",
    "    def voting(self, y_hats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_hats (ndarray): [N] ndarray of data\n",
    "        Returns:\n",
    "            y_final : int, final prediction of the \n",
    "        \"\"\"\n",
    "        #TODO: Implement majority voting scheme and incase of ties return random label\n",
    "        tot = 0\n",
    "        for xi in y_hats:\n",
    "            tot += sum(xi)\n",
    "            \n",
    "        if tot < 0:\n",
    "            y_f = -1\n",
    "        elif tot > 0:\n",
    "            y_f = 1\n",
    "        else:\n",
    "            y_f = int(np.random.choice([-1,1], size = 1))\n",
    "            \n",
    "        return y_f\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaggingClassifier for Handwritten Digit Recognition [10 points]\n",
    "***\n",
    "\n",
    "After you've successfully completed `BaggingClassifier` find the optimal values of `ratio`, `N` and `depth` using k-fold cross validation. You are allowed to use sklearn library to split your training data in folds. Use the data from `ThreesAndEights` class initialized variable `data`. Hyperparameter tuning as you may have noticed is very important in Machine Learning.  \n",
    "\n",
    "Justify why those values are optimal.\n",
    "\n",
    "Report accuracy on the validation data using the optimal parameter values.\n",
    "\n",
    "__Note__: This might take a little longer time than usual to run (i.e. several minutes). This is true for the ensemble methods you will implement below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_r = np.arange(5,25,5)\n",
    "r_r = np.arange(.1, 1, 0.4)\n",
    "d_r = np.arange(3,24,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N [ 5 10 15 20]\n",
      "ratio [0.1 0.5 0.9]\n",
      "depth [ 3 10 17]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "Num_k = 5\n",
    "\n",
    "print(\"N\", N_r)\n",
    "print(\"ratio\", r_r)\n",
    "print(\"depth\", d_r)\n",
    "\n",
    "kfold = KFold(Num_k, True, 555)\n",
    "D = []\n",
    "\n",
    "itera = 0\n",
    "\n",
    "for n in N_r:\n",
    "    for r in r_r:\n",
    "        for d in d_r:\n",
    "            itera += 1\n",
    "            param = (n, r, d)\n",
    "            error = 0\n",
    "            c_f_a = [] \n",
    "            for train, test in kfold.split(data.X_train):\n",
    "                trained_tree = BaggingClassifier(ratio = r, N = n, base = DecisionTreeClassifier(max_depth = d))\n",
    "                trained_tree.fit(data.X_train[train], data.y_train[train])\n",
    "                \n",
    "                y_pred = np.array(trained_tree.predict(data.X_train[test]))\n",
    "                y_actual = data.y_train[test]     \n",
    "                \n",
    "                total_errors = y_pred != y_actual\n",
    "                error = np.sum(total_errors)\n",
    "                percent_accuracy = 1 - error/len(y_actual)\n",
    "                c_f_a.append(percent_accuracy)\n",
    "\n",
    "            \n",
    "            average_accuracy = np.average(current_fold_accuracy)\n",
    "            D.append((average_accuracy, param))\n",
    "            print(itera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>__Expected accuracy is about 97%__</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy for : 0.98, the value of N: 15, with  ratio: 0.5, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 15, with  ratio: 0.9, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 20, with  ratio: 0.9, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 20, with  ratio: 0.5, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 20, with  ratio: 0.9, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 15, with  ratio: 0.9, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 10, with  ratio: 0.9, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 10, with  ratio: 0.5, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 20, with  ratio: 0.5, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 5, with  ratio: 0.9, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 15, with  ratio: 0.5, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 10, with  ratio: 0.9, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 5, with  ratio: 0.9, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 10, with  ratio: 0.5, and depth: 10\n",
      "Acuuracy for : 0.97, the value of N: 5, with  ratio: 0.5, and depth: 17\n",
      "Acuuracy for : 0.97, the value of N: 5, with  ratio: 0.5, and depth: 10\n",
      "Acuuracy for : 0.96, the value of N: 15, with  ratio: 0.1, and depth: 17\n",
      "Acuuracy for : 0.96, the value of N: 20, with  ratio: 0.1, and depth: 17\n",
      "Acuuracy for : 0.96, the value of N: 20, with  ratio: 0.1, and depth: 10\n",
      "Acuuracy for : 0.96, the value of N: 15, with  ratio: 0.1, and depth: 10\n",
      "Acuuracy for : 0.96, the value of N: 10, with  ratio: 0.1, and depth: 17\n",
      "Acuuracy for : 0.96, the value of N: 10, with  ratio: 0.1, and depth: 10\n",
      "Acuuracy for : 0.95, the value of N: 5, with  ratio: 0.1, and depth: 17\n",
      "Acuuracy for : 0.95, the value of N: 5, with  ratio: 0.1, and depth: 10\n",
      "Acuuracy for : 0.94, the value of N: 10, with  ratio: 0.1, and depth: 3\n",
      "Acuuracy for : 0.94, the value of N: 15, with  ratio: 0.1, and depth: 3\n",
      "Acuuracy for : 0.94, the value of N: 20, with  ratio: 0.1, and depth: 3\n",
      "Acuuracy for : 0.94, the value of N: 15, with  ratio: 0.9, and depth: 3\n",
      "Acuuracy for : 0.94, the value of N: 15, with  ratio: 0.5, and depth: 3\n",
      "Acuuracy for : 0.94, the value of N: 20, with  ratio: 0.5, and depth: 3\n",
      "Acuuracy for : 0.94, the value of N: 10, with  ratio: 0.5, and depth: 3\n",
      "Acuuracy for : 0.93, the value of N: 10, with  ratio: 0.9, and depth: 3\n",
      "Acuuracy for : 0.93, the value of N: 5, with  ratio: 0.1, and depth: 3\n",
      "Acuuracy for : 0.93, the value of N: 5, with  ratio: 0.9, and depth: 3\n",
      "Acuuracy for : 0.93, the value of N: 20, with  ratio: 0.9, and depth: 3\n",
      "Acuuracy for : 0.93, the value of N: 5, with  ratio: 0.5, and depth: 3\n"
     ]
    }
   ],
   "source": [
    "s_b_e = sorted(D, key = lambda tup: tup[0], reverse = True)\n",
    "    \n",
    "for e in s_b_e:\n",
    "    \n",
    "    print(\"Acuuracy for : %.2f, the value of N: %i, with  ratio: %.1f, and depth: %i\" %(e[0],e[1][0],e[1][1],e[1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing w/ chosen values from above\n",
    "\n",
    "f_N =15\n",
    "f_r = 0.5\n",
    "f_d = 17\n",
    "\n",
    "\n",
    "BaggedTr = BaggingClassifier(ratio= f_r,N= f_N,base=DecisionTreeClassifier(max_depth = f_d))\n",
    "\n",
    "BaggedTr.fit(data.X_train, data.y_train)\n",
    "Bagged_predic = BaggedTr.predict(data.X_valid)\n",
    "act = data.y_valid\n",
    "\n",
    "Bagged_t_errors = Bagged_predic != act\n",
    "B_error = np.sum(Bagged_t_errors)\n",
    "B_percent = 100*(1 - B_error/len(act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 15 trees, a ratio of 0.5, and a maximum depth of 17, I got an accuarcy of 98.0 on my test set.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using %i trees, a ratio of %.1f, and a maximum depth of %i, I got an accuarcy of %.1f on my test set.\" %(f_N, f_r, f_d, B_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I am  r aunning a nested for loop, examining many permutations of the three variables. To imporve the running time i could have used three different loops and kept values constant and varied each element one at a time. From my answer it can be understood that depth has the maximum influence on the result. A deeper tree results to more overfitting. Due to overfitting the tree gives better acuracy for training but doesnt perform well with testing data. I decided to use the depth as 17 as it was a value in between and I decided to use ratio as 50% to get equal split so that i can have equal training and testing data.I feel lower percent would give much better results. Number of trees have minimal effects on the results. I used 15 trees since it gave me the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Decision Tree [35 points]\n",
    "\n",
    "In this assignment you are going to implement a random decision tree using random vector method as discussed in the lecture.\n",
    "\n",
    "Best split: One that achieves maximum reduction in gini index across multiple candidate splits. (decided by `candidate_splits` attribute of the class `RandomDecisionTree`)\n",
    "\n",
    "Use `TreeNode` class as node abstraction to build the tree\n",
    "\n",
    "You are allowed to add new attributes in the `TreeNode` and `RandomDecisionTree` class - if that helps.\n",
    "\n",
    "Your tasks are as follows:\n",
    "* Implement `gini_index` method which takes in class labels as parameter and returns the gini impurity as measure of uncertainty\n",
    "\n",
    "* Implement `majority` method which picks the most frequent class label. In case of tie return any random class label\n",
    "\n",
    "* Implement `find_best_split` method which finds the random vector/hyperplane which causes most reduction in the gini index. \n",
    "\n",
    "* Implement `build_tree` method which uses `find_best_split` method to get the best random split vector for current set of training points. This vector partitions the training points into two sets, and you should call `build_tree` method on two partitioned sets and build left subtree and right subtree. Use `TreeNode` as abstraction for a node.\n",
    "\n",
    "> The method calls itself recursively to the generate left and right subtree till the point either `max_depth` is reached or no good random split is found.  When either of two cases is encountered, you should make that node as leaf and identify the label for that leaf to be the most frequent class (use `majority` method). Go through lecture slides for better understanding\n",
    "\n",
    "* Implement `predict` method which takes in multiple data points and returns final prediction for each one of those using the tree built. (`root` attribute of the class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.isLeaf = False\n",
    "        self.label = None\n",
    "        self.split_vector = None\n",
    "\n",
    "    def getLabel(self):\n",
    "        if not self.isLeaf:\n",
    "            raise Exception(\"Should not to do getLabel on a non-leaf node\")\n",
    "        return self.label\n",
    "    \n",
    "class RandomDecisionTree:\n",
    "            \n",
    "    def __init__(self, candidate_splits = 100, depth = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            candidate_splits (int) : number of random decision splits to test\n",
    "            depth (int) : maximum depth of the random decision tree\n",
    "        \"\"\"\n",
    "        self.candidate_splits = candidate_splits\n",
    "        self.depth = depth\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data\n",
    "            \n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X_train[:], y_train[:], 0)\n",
    "        return self\n",
    "        \n",
    "    def build_tree(self, X_train, y_train, height):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data\n",
    "            \n",
    "        \"\"\"\n",
    "        node = TreeNode()\n",
    "       \n",
    "        c_s = self.find_best_split(X_train, y_train)\n",
    "        if height == self.depth or len(y_train) < 2:             \n",
    "            node.isLeaf = True\n",
    "            node.label = self.majority(y_train)\n",
    "            return node\n",
    "        \n",
    "        height += 1\n",
    "        l_s, r_s = [], []\n",
    "        index = 0\n",
    "        \n",
    "        for pt in X_train:\n",
    "            side = np.dot(pt, c_s)\n",
    "            if side < 0:\n",
    "                l_s.append(index)\n",
    "            else:\n",
    "                r_s.append(index)\n",
    "            index += 1            \n",
    "        l_p, r_p = np.array(l_s), np.array(r_s)\n",
    "        \n",
    "        if len(l_p) == 0 or len(r_p) == 0:\n",
    "            node.isLeaf = True\n",
    "            node.label = self.majority(y_train)\n",
    "            return node            \n",
    "        \n",
    "        l_b = self.build_tree(X_train[l_p], y_train[l_p], height)\n",
    "        r_b = self.build_tree(X_train[r_p], y_train[r_p], height)\n",
    "       \n",
    "        node.left = l_b\n",
    "        node.right = r_b\n",
    "        node.split_vector = c_s\n",
    "        return node\n",
    "    \n",
    "    def find_best_split(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data\n",
    "            \n",
    "        \"\"\"\n",
    "        u_i = self.gini_index(y_train)\n",
    "        T_P = len(y_train)\n",
    "        \n",
    "        dims = len(X_train[0])\n",
    "        n_c = self.candidate_splits\n",
    "        max_g = float(-np.inf)\n",
    "        split_vector = None\n",
    "        for x in range(n_c):\n",
    "            candidate_split = np.random.uniform(-1,1,dims)\n",
    "            left, right = [], []\n",
    "            index = 0\n",
    "            for pt in X_train:\n",
    "                side = np.dot(pt, candidate_split)\n",
    "                if side < 0:\n",
    "                    left.append(index)\n",
    "                else:\n",
    "                    right.append(index)\n",
    "                index += 1\n",
    "\n",
    "            left, right = np.array(left), np.array(right)           \n",
    "            p_Left, p_Right = len(left)/T_P, len(right)/T_P\n",
    "          \n",
    "            if p_Left > 0:\n",
    "                u_Left = self.gini_index(y_train[left])\n",
    "            else:\n",
    "                u_Left = 0\n",
    "                \n",
    "            if p_Right > 0:\n",
    "                u_Right = self.gini_index(y_train[right])\n",
    "            else:\n",
    "                u_Right = 0\n",
    "                \n",
    "            Gain_of_split = u_i - (p_Left*u_Left) - (p_Right*u_Right)\n",
    "            if Gain_of_split > max_g:\n",
    "                split_vector = candidate_split\n",
    "                max_g = Gain_of_split\n",
    "        # your logic here\n",
    "        return split_vector\n",
    "        \n",
    "            \n",
    "        \n",
    "    def gini_index(self, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y (ndarray): [n_samples] ndarray of data\n",
    "        \"\"\"\n",
    "        l_c = {}\n",
    "\n",
    "        for label in np.unique(y):\n",
    "            l_c.update( {label : list(y).count(label)})\n",
    "\n",
    "        T_P = len(y)\n",
    "\n",
    "        try:\n",
    "            P = l_c[-1]\n",
    "        except:\n",
    "            P = l_c[1]\n",
    "\n",
    "        u = 2 * (P/T_P) * (1 - (P/T_P))\n",
    "        return(u)\n",
    "\n",
    "    \n",
    "    def majority(self, y):\n",
    "        \"\"\"\n",
    "        Return the major class in ndarray y\n",
    "        \"\"\"\n",
    "        l_c = {}\n",
    "        for label in np.unique(y):\n",
    "            l_c.update( {label : list(y).count(label)})\n",
    "            \n",
    "        m_c = 0\n",
    "        for label in l_c:\n",
    "            if l_c[label] > m_c:\n",
    "                m_l = label\n",
    "                m_c = l_c[label]\n",
    "        return m_l\n",
    "                    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        BaggingClassifier prediction for new data points in X\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns:\n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "        yhat = []\n",
    "        \n",
    "        for point in X:\n",
    "            node = self.root            \n",
    "            while node.isLeaf == False:\n",
    "                direction = np.dot(point, node.split_vector)\n",
    "                if direction < 0:\n",
    "                    node = node.left\n",
    "\n",
    "                else:\n",
    "                    node = node.right\n",
    "   \n",
    "            y_pred = node.getLabel()\n",
    "            yhat.append(y_pred)\n",
    "                \n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomDecisionTree for Handwritten Digit Recognition\n",
    "\n",
    "After you've successfully completed `RandomDecisionTree`, and train using the default values in the constructor and report accuracy on the test set. Use the data from `ThreesAndEights` class initialized variable `data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_t = RandomDecisionTree()\n",
    "\n",
    "r_t.fit(data.X_train, data.y_train)\n",
    "predict = r_t.predict(data.X_valid)\n",
    "act = data.y_valid\n",
    "\n",
    "total_e = predict != act\n",
    "err = np.sum(total_e)\n",
    "perce = 100*(1 - err/len(data.y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of my random decision tree is 90.1 percent. \n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of my random decision tree is %.1f percent. \" %(perce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>__Expected accuracy is about 90%__</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest [20 points]\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "Random forest trains random decision trees on bootstrapped training points. Thus, you can implementation of methods (`bootstrap`, `predict`) from `BaggingClassifier` class directly. Only difference being, you have to use the `RandomDecisionTree` as base which you implemented previously instead of sklearn's implementation of `DecisionTreeClassifier`). Implement the `fit` method in the class below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggingClassifier):\n",
    "    def __init__(self, ratio = 0.20, N = 20, max_depth = 10, candidate_splits = 100):\n",
    "        self.ratio = ratio\n",
    "        self.N = N\n",
    "        self.learners = []\n",
    "        self.candidate_splits = candidate_splits\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train Bagging Ensemble Classifier on data\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        for i in range(0,self.N): \n",
    "            r_t = RandomDecisionTree(candidate_splits = self.candidate_splits, depth = self.max_depth)\n",
    "            bt_X, bt_y = self.boostrap(X_train, y_train)\n",
    "            self.learners.append(r_t.fit(bt_X, bt_y))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest for Handwritten Digit Recognition [10 points]\n",
    "***\n",
    "\n",
    "After you've successfully completed `RandomForest` find the optimal values of `ratio`, `N`, `candidate_splits` and `depth` using k-fold cross validation on. Feel free to use sklearn library to split your training data. Use the data from `ThreesAndEights` class intialized variable `data`. \n",
    "\n",
    "Justify why those values are optimal.\n",
    "\n",
    "Report best accuracy on the testing data using those optimal parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_c = 25\n",
    "r_c = .2\n",
    "d_c = 4\n",
    "c_c = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N [ 5 10 15 20]\n",
      "ratio 0.2\n",
      "depth 4\n",
      "candidates 100\n"
     ]
    }
   ],
   "source": [
    "# by varying N\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "Num_k = 5\n",
    "\n",
    "N_r = np.arange(5, 25, 5)\n",
    "r = r_c\n",
    "d = d_c\n",
    "c = c_c\n",
    "print(\"N\", N_r)\n",
    "print(\"ratio\", r)\n",
    "print(\"depth\", d)\n",
    "print(\"candidates\", c)\n",
    "\n",
    "kfold = KFold(Num_k, True, 555)\n",
    "N_s_f = []\n",
    "\n",
    "\n",
    "for n in N_r:\n",
    "    e = 0\n",
    "    c_f_a = []\n",
    "    params = (n,r,d,c)\n",
    "    for train, test in kfold.split(data.X_train):\n",
    "        forest = RandomForest(ratio = r, N = n, max_depth = d, candidate_splits = c )\n",
    "        forest.fit(data.X_train[train], data.y_train[train])\n",
    "\n",
    "        y_pred = np.array(forest.predict(data.X_train[test]))\n",
    "        y_actual = data.y_train[test]     \n",
    "\n",
    "        t_e = y_pred != y_actual\n",
    "        e = np.sum(t_e)\n",
    "        per_acc = 1 - e/len(y_actual)\n",
    "        c_f_a.append(per_acc)\n",
    "\n",
    "\n",
    "    N_a_a_f = np.average(c_f_a)\n",
    "    N_s_f.append((N_a_a_f, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93, N: 20.000000\n",
      "Accuracy: 0.93, N: 15.000000\n",
      "Accuracy: 0.93, N: 10.000000\n",
      "Accuracy: 0.91, N: 5.000000\n"
     ]
    }
   ],
   "source": [
    "s_b_e1 = sorted(N_s_f, key = lambda tup: tup[0], reverse = True)\n",
    "    \n",
    "for e in s_b_e1:\n",
    "  \n",
    "    print(\"Accuracy: %.2f, N: %1f\" %(e[0],e[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio [0.1 0.5 0.9]\n",
      "N 25\n",
      "depth 4\n",
      "candidates 100\n"
     ]
    }
   ],
   "source": [
    "#Ratio varied\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "Num_k = 5\n",
    "\n",
    "\n",
    "r_r = np.arange(.1, 1, .4)\n",
    "n = N_c\n",
    "d = d_c\n",
    "c = c_c\n",
    "print(\"ratio\",r_r)\n",
    "print(\"N\", n)\n",
    "print(\"depth\", d)\n",
    "print(\"candidates\", c)\n",
    "\n",
    "kfold = KFold(Num_k, True, 555)\n",
    "r_s_f = []\n",
    "\n",
    "for r in r_r:\n",
    "    e = 0\n",
    "    c_f_a = [] \n",
    "    for train, test in kfold.split(data.X_train):\n",
    "        forest = RandomForest(ratio = r, N = n, max_depth = d, candidate_splits = c )\n",
    "        forest.fit(data.X_train[train], data.y_train[train])\n",
    "\n",
    "        y_pred = np.array(forest.predict(data.X_train[test]))\n",
    "        y_actual = data.y_train[test] \n",
    "        \n",
    "        t_e = y_pred != y_actual\n",
    "        e = np.sum(t_e)\n",
    "        per_acc = 1 - e/len(y_actual)\n",
    "        c_f_a.append(per_acc)\n",
    "\n",
    "\n",
    "    r_a_a_f = np.average(c_f_a)\n",
    "    r_s_f.append((r_a_a_f,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94,r: 0.9\n",
      "Accuracy: 0.93,r: 0.5\n",
      "Accuracy: 0.93,r: 0.1\n"
     ]
    }
   ],
   "source": [
    "s_b_e2 = sorted(r_s_f, key = lambda tup: tup[0], reverse = True)\n",
    "    \n",
    "for e in s_b_e2:\n",
    "   \n",
    "    print(\"Accuracy: %.2f,r: %.1f\" %(e[0],e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth [ 3 10 17]\n",
      "ratio 0.2\n",
      "N 25\n",
      "candidates 100\n"
     ]
    }
   ],
   "source": [
    "#depth varied\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "Num_k = 5\n",
    "\n",
    "d_r = np.arange(3, 24, 7)\n",
    "n = N_c\n",
    "r = r_c\n",
    "c = c_c\n",
    "\n",
    "print(\"depth\", d_r)\n",
    "print(\"ratio\",r)\n",
    "print(\"N\", n)\n",
    "print(\"candidates\", c)\n",
    "\n",
    "\n",
    "kfold = KFold(Num_k, True, 555)\n",
    "d_s_f = []\n",
    "\n",
    "for d in d_r:\n",
    "    e = 0\n",
    "    c_f_a = [] \n",
    "    for train, test in kfold.split(data.X_train):\n",
    "        forest = RandomForest(ratio = r, N = n, max_depth = d, candidate_splits = c )\n",
    "        forest.fit(data.X_train[train], data.y_train[train])\n",
    "\n",
    "        y_pred = np.array(forest.predict(data.X_train[test]))\n",
    "        y_actual = data.y_train[test]      \n",
    "\n",
    "        t_e = y_pred != y_actual\n",
    "        e = np.sum(t_e)\n",
    "        per_acc = 1 - e/len(y_actual)\n",
    "        c_f_a.append(per_acc)\n",
    "\n",
    "\n",
    "    d_a_a_f = np.average(c_f_a)\n",
    "    d_s_f.append((d_a_a_f, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96, d: 17\n",
      "Accuracy: 0.96, d: 10\n",
      "Accuracy: 0.92, d: 3\n"
     ]
    }
   ],
   "source": [
    "s_b_e3 = sorted(d_s_f, key = lambda tup: tup[0], reverse = True)\n",
    "    \n",
    "for e in s_b_e3:\n",
    "    print(\"Accuracy: %.2f, d: %i\" %(e[0],e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates [10 20 30 40 50 60 70 80 90]\n",
      "depth 4\n",
      "ratio 0.2\n",
      "N 25\n"
     ]
    }
   ],
   "source": [
    "#number of candidates varied\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "Num_k = 5\n",
    "\n",
    "c_r = np.arange(10, 100, 10)\n",
    "n = N_c\n",
    "r = r_c\n",
    "d = d_c\n",
    "print(\"candidates\", c_r)\n",
    "print(\"depth\", d)\n",
    "print(\"ratio\", r)\n",
    "print(\"N\", n)\n",
    "\n",
    "\n",
    "kfold = KFold(Num_k, True, 555)\n",
    "c_s_f = []\n",
    "\n",
    "for c in c_r:\n",
    "    e = 0\n",
    "    c_f_a = [] \n",
    "    for train, test in kfold.split(data.X_train):\n",
    "        forest = RandomForest(ratio = r, N = n, max_depth = d, candidate_splits = c )\n",
    "        forest.fit(data.X_train[train], data.y_train[train])\n",
    "\n",
    "        y_pred = np.array(forest.predict(data.X_train[test]))\n",
    "        y_actual = data.y_train[test]      \n",
    "\n",
    "        t_e = y_pred != y_actual\n",
    "        e = np.sum(t_e)\n",
    "        per_acc = 1 - e/len(y_actual)\n",
    "        c_f_a.append(per_acc)\n",
    "\n",
    "\n",
    "    c_a_a_f = np.average(c_f_a)\n",
    "    c_s_f.append((c_a_a_f, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93,c: 90\n",
      "Accuracy: 0.93,c: 70\n",
      "Accuracy: 0.93,c: 80\n",
      "Accuracy: 0.93,c: 60\n",
      "Accuracy: 0.92,c: 40\n",
      "Accuracy: 0.92,c: 50\n",
      "Accuracy: 0.92,c: 30\n",
      "Accuracy: 0.92,c: 20\n",
      "Accuracy: 0.91,c: 10\n"
     ]
    }
   ],
   "source": [
    "s_b_e4 = sorted(c_s_f, key = lambda tup: tup[0], reverse = True)\n",
    "    \n",
    "for e in s_b_e4:\n",
    "    print(\"Accuracy: %.2f,c: %i\" %(e[0],e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15Number of  trees,  for ratio of 0.9, with depth of 17, and number of candidates 80, the accuracy of 96.3 on test set.\n"
     ]
    }
   ],
   "source": [
    "f_N_f = 15\n",
    "f_r_f = 0.9\n",
    "f_d_f = 17\n",
    "f_c_f = 80\n",
    "\n",
    "\n",
    "FF = RandomForest(ratio = f_r_f, N = f_N_f, max_depth = f_d_f, candidate_splits = f_c_f)\n",
    "\n",
    "\n",
    "FF.fit(data.X_train, data.y_train)\n",
    "Fp = FF.predict(data.X_valid)\n",
    "actual = data.y_valid\n",
    "\n",
    "Fte = Fp != actual\n",
    "Fe = np.sum(Fte)\n",
    "Fpercent = 100*(1 - Fe/len(actual))\n",
    "print(\" %iNumber of  trees,  for ratio of %.1f, with depth of %i, and number of candidates %i, the accuracy of %.1f on test set.\" %(f_N_f, f_r_f, f_d_f,f_c_f, Fpercent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>__Expected accuracy is about 97%__</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15Number of  trees,  for ratio of 0.9, with depth of 17, and number of candidates 60, the accuracy of 97.1 on test set.\n"
     ]
    }
   ],
   "source": [
    "f_N_f = 15\n",
    "f_r_f = 0.9\n",
    "f_d_f = 17\n",
    "f_c_f = 60\n",
    "\n",
    "\n",
    "FF = RandomForest(ratio = f_r_f, N = f_N_f, max_depth = f_d_f, candidate_splits = f_c_f)\n",
    "\n",
    "\n",
    "FF.fit(data.X_train, data.y_train)\n",
    "Fp = FF.predict(data.X_valid)\n",
    "actual = data.y_valid\n",
    "\n",
    "Fte = Fp != actual\n",
    "Fe = np.sum(Fte)\n",
    "Fpercent = 100*(1 - Fe/len(actual))\n",
    "print(\" %iNumber of  trees,  for ratio of %.1f, with depth of %i, and number of candidates %i, the accuracy of %.1f on test set.\" %(f_N_f, f_r_f, f_d_f,f_c_f, Fpercent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20Number of  trees,  for ratio of 0.9, with depth of 17, and number of candidates 70, the accuracy of 96.8 on test set.\n"
     ]
    }
   ],
   "source": [
    "f_N_f = 20\n",
    "f_r_f = 0.9\n",
    "f_d_f = 17\n",
    "f_c_f = 70\n",
    "\n",
    "\n",
    "FF = RandomForest(ratio = f_r_f, N = f_N_f, max_depth = f_d_f, candidate_splits = f_c_f)\n",
    "\n",
    "\n",
    "FF.fit(data.X_train, data.y_train)\n",
    "Fp = FF.predict(data.X_valid)\n",
    "actual = data.y_valid\n",
    "\n",
    "Fte = Fp != actual\n",
    "Fe = np.sum(Fte)\n",
    "Fpercent = 100*(1 - Fe/len(actual))\n",
    "print(\" %iNumber of  trees,  for ratio of %.1f, with depth of %i, and number of candidates %i, the accuracy of %.1f on test set.\" %(f_N_f, f_r_f, f_d_f,f_c_f, Fpercent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I ran four different loops instead of using nested for loops similiar to baggingclassifier.I chose my number of trees to be N = 20, which is close to the 25 I used previously. I used a ratio of 90% , since this performed well and seems to give good variability. I used a depth again of 17 again, for the same reasons as explained above. I tested for different candidate values initiallly i used 80 and then changed to 60. Lower candidate value gave better results rather than higher value. I later changed the Number of trees and used a value between both the candidate value and got better results than higher candidte value with lower number of trees but lower accuracy than lower candidate value and lower number of trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
